{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook is for trying out what research I have read and my ideas in this [Notion](https://www.notion.so/Model-evaluation-170d3cfe027d80889de3cfbb35b531a4).\n",
    "\n",
    "I played around with a Mistral Large model to build a mathematics chatbot. It is simple and I prompt engineered it. I might change the model to a smaller and cheaper one, but I still want to use a model that is considerd good enough. My plan is to use three small open source models to evaluate the mahtematics model. I'll also use different models for the mathematics chatbot and for the evaluation models to get realible results.\n",
    "\n",
    "I'll use this [dataset](https://github.com/google-deepmind/mathematics_dataset?tab=readme-ov-file) as a test data set. I'll select 50 mathematics questiosn from the dataset. Then I'll ask the mathematicsbot the same questions. \n",
    "Then I'll let: \n",
    "- First a Mistral Large model compare the answers. Same model as in the mathematicsbot but different prompt engineering. \n",
    "    - This is not a good way to evaluate models, but I wish to try it. Espeacilly when it is only 50 questions it is manageble for me to compare the answers.\n",
    "- Then I'll let three smaller open-source models compare the answers.\n",
    "- After that I'll add a fourth model to compare the answers of the three open-open source model to give a final evaluation about the mathematics chat bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mistralai in ./venv/lib/python3.11/site-packages (1.2.6)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in ./venv/lib/python3.11/site-packages (from mistralai) (0.2.2)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in ./venv/lib/python3.11/site-packages (from mistralai) (0.27.2)\n",
      "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in ./venv/lib/python3.11/site-packages (from mistralai) (1.0.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in ./venv/lib/python3.11/site-packages (from mistralai) (2.10.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in ./venv/lib/python3.11/site-packages (from mistralai) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-inspect<0.10.0,>=0.9.0 in ./venv/lib/python3.11/site-packages (from mistralai) (0.9.0)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (4.8.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (1.0.7)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (3.10)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->mistralai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.8.2->mistralai) (1.17.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./venv/lib/python3.11/site-packages (from typing-inspect<0.10.0,>=0.9.0->mistralai) (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: python-dotenv in ./venv/lib/python3.11/site-packages (1.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: mathematics_dataset in ./venv/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: absl-py>=0.1.0 in ./venv/lib/python3.11/site-packages (from mathematics_dataset) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.10 in ./venv/lib/python3.11/site-packages (from mathematics_dataset) (1.24.3)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.11/site-packages (from mathematics_dataset) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.2 in ./venv/lib/python3.11/site-packages (from mathematics_dataset) (1.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.11/site-packages (from sympy>=1.2->mathematics_dataset) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pip in ./venv/lib/python3.11/site-packages (23.1.2)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement install (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for install\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: numpy==1.24.3 in ./venv/lib/python3.11/site-packages (1.24.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install mistralai\n",
    "!pip3 install python-dotenv\n",
    "!pip3 install mathematics_dataset\n",
    "!pip3 install pip install sympy==1.6\n",
    "!pip3 install numpy==1.24.3\n",
    "!pip3 install transformers==4.48.0\n",
    "!pip3 install tiktoken==0.8.0\n",
    "!pip3 install protobuf==5.29.3\n",
    "!pip3 install sentencepiece==0.2.0\n",
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've used the following prompt for the Mistral Large 2.1 model\n",
    "\"Your task is to help solve math problems. Generate three answers that helps solving the problem but doesn't give the solution. When generating the 4th answer to the problem you can give the solution. When you don't know the answer say that you don't know the answer. Write the mathematical part of the answer in latex.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. First, simplify the equation by combining like terms on the right side.\n",
      "   \\[\n",
      "   0 = 4a + 12\n",
      "   \\]\n",
      "   The answer is not final yet.\n",
      "\n",
      "2. To isolate the term with \\( a \\), subtract 12 from both sides of the equation.\n",
      "   \\[\n",
      "   -12 = 4a\n",
      "   \\]\n",
      "   The answer is not final yet.\n",
      "\n",
      "3. Divide both sides by 4 to solve for \\( a \\).\n",
      "   \\[\n",
      "   a = \\frac{-12}{4}\n",
      "   \\]\n",
      "   The answer is not final yet.\n",
      "\n",
      "4. Simplify the right side to find the value of \\( a \\).\n",
      "   \\[\n",
      "   a = -3\n",
      "   \\]\n",
      "   The answer is -3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from mistralai import Mistral\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "agent_id = os.environ[\"MISTRAL_AGENT_ID\"]\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "chat_response = client.agents.complete(\n",
    "    agent_id= agent_id,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Solve 0 = 4*a + 19 - 7 for a.\"\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from mistralai import Mistral\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key and agent ID\n",
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "agent_id = os.environ[\"MISTRAL_AGENT_ID\"]\n",
    "\n",
    "# Initialize Mistral client\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "# Load questions from the JSON file\n",
    "input_file = \"questions.json\"  # Replace with your input file name\n",
    "output_file = \"model_answers.json\"  # Output file name\n",
    "\n",
    "with open(input_file, \"r\") as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over the questions\n",
    "for question in questions:\n",
    "    try:\n",
    "        # Ask the question to the model\n",
    "        chat_response = client.agents.complete(\n",
    "            agent_id=agent_id,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": question\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "        # Extract the model's answer\n",
    "        answer = chat_response.choices[0].message.content.strip()\n",
    "\n",
    "        # Append the question and answer to the results\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer: {answer}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question: {question}\")\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Save the results to the output file\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used the [mathematics_dataset](https://github.com/google-deepmind/mathematics_dataset) from Google DeepMind that will work as a test set. The main issue is that I don't know if the Mistral Large model that is used for the mathematics model have been trained on this data. There is a great chans for that. \n",
    "\n",
    "Never the less, I made a json file called questions_and_answers of the [mathematics_dataset](https://github.com/google-deepmind/mathematics_dataset) so that I can easily separet the questions from the answers. Ideally I would run all the questions through the mathematics model, but due to budget questions that won't be possible. I'll choose around 50 questions from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to questions_and_answers.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def clean_ansi(text):\n",
    "    \"\"\"Remove ANSI color codes from text\"\"\"\n",
    "    ansi_pattern = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\\\[[0-?]*[ -/]*[@-~])')\n",
    "    return ansi_pattern.sub('', text)\n",
    "\n",
    "def process_qa_data(raw_text, output_file=\"questions_and_answers.json\"):\n",
    "    \"\"\"Process raw text into question-answer pairs and save to JSON\"\"\"\n",
    "    qa_pairs = []\n",
    "    \n",
    "    # Split into lines and process each line\n",
    "    lines = raw_text.split('\\n') if isinstance(raw_text, str) else raw_text\n",
    "    \n",
    "    current_section = None\n",
    "    current_question = []\n",
    "    previous_answer = None\n",
    "    \n",
    "    for line in lines:\n",
    "        # Skip empty lines\n",
    "        if not line.strip():\n",
    "            continue\n",
    "            \n",
    "        # Check if this is a section header\n",
    "        if line.startswith('\\u001b[1m') and '\\u001b[92m' not in line:\n",
    "            current_section = clean_ansi(line.strip())\n",
    "            current_question = []\n",
    "            previous_answer = None\n",
    "            continue\n",
    "            \n",
    "        # Skip log messages\n",
    "        if line.startswith('W') and ']' in line:\n",
    "            continue\n",
    "        \n",
    "        # If line contains an answer (marked by green color code)\n",
    "        if '\\u001b[92m' in line:\n",
    "            parts = line.split('\\u001b[92m')\n",
    "            \n",
    "            # Get the question part (if any)\n",
    "            question_part = parts[0].strip()\n",
    "            \n",
    "            # If there's a reset code in the question part, it contains a previous answer\n",
    "            if '\\u001b[0m' in question_part:\n",
    "                q_parts = question_part.split('\\u001b[0m')\n",
    "                if len(q_parts) > 1 and previous_answer is not None:\n",
    "                    # Complete the previous answer\n",
    "                    qa_pairs[-1][\"answer\"] += ' ' + clean_ansi(q_parts[0].strip())\n",
    "                    # Update the question part to exclude the previous answer\n",
    "                    question_part = q_parts[1].strip()\n",
    "            \n",
    "            # Add any new question part to current question\n",
    "            if question_part:\n",
    "                current_question.append(question_part)\n",
    "            \n",
    "            # Get the answer\n",
    "            answer = parts[1].strip()\n",
    "            if '\\u001b[0m' in answer:\n",
    "                answer = answer.split('\\u001b[0m')[0].strip()\n",
    "            \n",
    "            # Combine all question parts\n",
    "            full_question = ' '.join(current_question).strip()\n",
    "            \n",
    "            # Add the new QA pair\n",
    "            if answer:\n",
    "                qa_pairs.append({\n",
    "                    \"section\": clean_ansi(current_section) if current_section else None,\n",
    "                    \"question\": full_question,\n",
    "                    \"answer\": clean_ansi(answer)\n",
    "                })\n",
    "                previous_answer = answer\n",
    "            \n",
    "            # Reset question accumulator\n",
    "            current_question = []\n",
    "        else:\n",
    "            # If line doesn't contain an answer, check if it contains a previous answer completion\n",
    "            if '\\u001b[0m' in line:\n",
    "                parts = line.split('\\u001b[0m')\n",
    "                if previous_answer is not None:\n",
    "                    # Add to previous answer\n",
    "                    qa_pairs[-1][\"answer\"] += ' ' + clean_ansi(parts[0].strip())\n",
    "                    # Add remaining part to current question\n",
    "                    if len(parts) > 1 and parts[1].strip():\n",
    "                        current_question.append(parts[1].strip())\n",
    "            else:\n",
    "                # Regular question part\n",
    "                current_question.append(line.strip())\n",
    "    \n",
    "    # Filter out any entries with empty questions\n",
    "    qa_pairs = [pair for pair in qa_pairs if pair[\"answer\"] and pair[\"question\"]]\n",
    "    \n",
    "    # Save to JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(qa_pairs, f, indent=2)\n",
    "    \n",
    "    print(f\"Output saved to {output_file}\")\n",
    "    return qa_pairs\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Get the output from mathematics_dataset.generate\n",
    "    output = !python -m mathematics_dataset.generate\n",
    "    \n",
    "    # Process the output\n",
    "    qa_data = process_qa_data(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to clean_qa_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def clean_specific_ansi(text):\n",
    "    \"\"\"Remove specific ANSI codes\"\"\"\n",
    "    if text:\n",
    "        return text.replace('\\u001b[1m', '').replace('\\u001b[0m', '')\n",
    "    return text\n",
    "\n",
    "def clean_json_file(input_file=\"questions_and_answers.json\", output_file=\"clean_qa_data.json\"):\n",
    "    # Read the JSON file\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Clean specific ANSI codes from each field\n",
    "    clean_data = []\n",
    "    for item in data:\n",
    "        clean_item = {\n",
    "            \"section\": clean_specific_ansi(item[\"section\"]),\n",
    "            \"question\": clean_specific_ansi(item[\"question\"]),\n",
    "            \"answer\": clean_specific_ansi(item[\"answer\"])\n",
    "        }\n",
    "        clean_data.append(clean_item)\n",
    "    \n",
    "    # Write the cleaned data back to a new JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(clean_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Cleaned data saved to {output_file}\")\n",
    "    return clean_data\n",
    "\n",
    "# Run the cleaning\n",
    "clean_data = clean_json_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON data from the file\n",
    "with open('clean_qa_data.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Extract only the questions\n",
    "questions = [entry[\"question\"] for entry in data]\n",
    "\n",
    "# Optionally, you can save the questions to a new JSON file\n",
    "with open('questions.json', 'w') as json_file:\n",
    "    json.dump(questions, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Sample data to try without extra costs\n",
    "# Load the JSON data from the file\n",
    "with open('sample_clean_qa_data.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Extract only the questions\n",
    "questions = [entry[\"question\"] for entry in data]\n",
    "\n",
    "# Optionally, you can save the questions to a new JSON file\n",
    "with open('sample_questions.json', 'w') as json_file:\n",
    "    json.dump(questions, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding evaluation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google T5 model (failed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try \"google/t5-small-ssm-nq\" cannot classify answers correctly when the Mistral Large model gives the output in the form \"The answer is 4\" and it compares it to only the mathematical answer \"4\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is 2 + 2?', 'expected_answer': '4', 'mistral_answer': 'The answer is 4', 'model_judgment': 'incorrect answers', 'mathematical_correct': False}\n",
      "{'question': 'What is 3 * 3?', 'expected_answer': '9', 'mistral_answer': 'When multiplying, 3 * 3 equals 9', 'model_judgment': 'incorrect answers', 'mathematical_correct': False}\n",
      "{'question': 'What is 10 - 3?', 'expected_answer': '7', 'mistral_answer': 'Subtracting, we get 6', 'model_judgment': 'incorrect answers', 'mathematical_correct': False}\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "from sympy import sympify, Eq, simplify\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "model_name = \"google/t5-small-ssm-nq\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# Example datasets\n",
    "dataset = [\n",
    "    {\"question\": \"What is 2 + 2?\", \"expected_answer\": \"4\", \"mistral_answer\": \"The answer is 4\"},\n",
    "    {\"question\": \"What is 3 * 3?\", \"expected_answer\": \"9\", \"mistral_answer\": \"When multiplying, 3 * 3 equals 9\"},\n",
    "    {\"question\": \"What is 10 - 3?\", \"expected_answer\": \"7\", \"mistral_answer\": \"Subtracting, we get 6\"}\n",
    "]\n",
    "\n",
    "# Function to validate correctness using the model\n",
    "def validate_correctness(question, mistral_answer, expected_answer):\n",
    "    def extract_math(expr):\n",
    "        \"\"\"\n",
    "        Extract and simplify the mathematical part of an expression.\n",
    "        Returns the simplified math expression or None if parsing fails.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return simplify(sympify(expr))\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    # Use T5 to classify the correctness of the Mistral model's output\n",
    "    prompt = (\n",
    "        f\"Evaluate if the given answer is correct:\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Mistral's Answer: {mistral_answer}\\n\"\n",
    "        f\"Expected Answer: {expected_answer}\\n\"\n",
    "        f\"Respond with 'Correct' or 'Incorrect'.\"\n",
    "    )\n",
    "\n",
    "    # Generate T5's judgment\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs.input_ids, max_length=10)\n",
    "    model_judgment = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Also validate mathematically\n",
    "    mistral_expr = extract_math(mistral_answer)\n",
    "    expected_expr = extract_math(expected_answer)\n",
    "    mathematical_correct = mistral_expr == expected_expr\n",
    "\n",
    "    return {\n",
    "        \"model_judgment\": model_judgment,\n",
    "        \"mathematical_correct\": mathematical_correct\n",
    "    }\n",
    "\n",
    "# Evaluate the dataset\n",
    "results = []\n",
    "for item in dataset:\n",
    "    result = validate_correctness(\n",
    "        question=item[\"question\"],\n",
    "        mistral_answer=item[\"mistral_answer\"],\n",
    "        expected_answer=item[\"expected_answer\"]\n",
    "    )\n",
    "    results.append({**item, **result})\n",
    "\n",
    "# Print results\n",
    "for res in results:\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand it can do a direct comparisson. So when it is an exact match it can compare the answers correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is 2 + 2?', 'expected_answer': '4', 'mistral_answer': '4', 'model_judgment': 'incorrect answers', 'mathematical_correct': True}\n",
      "{'question': 'What is 3 * 3?', 'expected_answer': '9', 'mistral_answer': '9', 'model_judgment': 'correction', 'mathematical_correct': True}\n",
      "{'question': 'What is 10 - 3?', 'expected_answer': '7', 'mistral_answer': '6', 'model_judgment': 'Mistral', 'mathematical_correct': False}\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "from sympy import sympify, Eq, simplify\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "model_name = \"google/t5-small-ssm-nq\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# Example datasets\n",
    "dataset = [\n",
    "    {\"question\": \"What is 2 + 2?\", \"expected_answer\": \"4\", \"mistral_answer\": \"4\"},\n",
    "    {\"question\": \"What is 3 * 3?\", \"expected_answer\": \"9\", \"mistral_answer\": \"9\"},\n",
    "    {\"question\": \"What is 10 - 3?\", \"expected_answer\": \"7\", \"mistral_answer\": \"6\"}\n",
    "]\n",
    "\n",
    "# Function to validate correctness using the model\n",
    "def validate_correctness(question, mistral_answer, expected_answer):\n",
    "    def extract_math(expr):\n",
    "        \"\"\"\n",
    "        Extract and simplify the mathematical part of an expression.\n",
    "        Returns the simplified math expression or None if parsing fails.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return simplify(sympify(expr))\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    # Use T5 to classify the correctness of the Mistral model's output\n",
    "    prompt = (\n",
    "        f\"Evaluate if the given answer is correct:\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Mistral's Answer: {mistral_answer}\\n\"\n",
    "        f\"Expected Answer: {expected_answer}\\n\"\n",
    "        f\"Respond with 'Correct' or 'Incorrect'.\"\n",
    "    )\n",
    "\n",
    "    # Generate T5's judgment\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs.input_ids, max_length=10)\n",
    "    model_judgment = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Also validate mathematically\n",
    "    mistral_expr = extract_math(mistral_answer)\n",
    "    expected_expr = extract_math(expected_answer)\n",
    "    mathematical_correct = mistral_expr == expected_expr\n",
    "\n",
    "    return {\n",
    "        \"model_judgment\": model_judgment,\n",
    "        \"mathematical_correct\": mathematical_correct\n",
    "    }\n",
    "\n",
    "# Evaluate the dataset\n",
    "results = []\n",
    "for item in dataset:\n",
    "    result = validate_correctness(\n",
    "        question=item[\"question\"],\n",
    "        mistral_answer=item[\"mistral_answer\"],\n",
    "        expected_answer=item[\"expected_answer\"]\n",
    "    )\n",
    "    results.append({**item, **result})\n",
    "\n",
    "# Print results\n",
    "for res in results:\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use regulatory expressions to remove symbols, it removes too much or too little so it it not a viable solution either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is 2 + 2?', 'expected_answer': '4', 'mistral_answer': 'The answer is 4', 'model_judgment': 'incorrect answers', 'mathematical_correct': True}\n",
      "{'question': 'What is 3 * 3?', 'expected_answer': '9', 'mistral_answer': 'When multiplying, 3 * 3 equals 9', 'model_judgment': 'incorrect answers', 'mathematical_correct': False}\n",
      "{'question': 'What is 10 - 3?', 'expected_answer': '7', 'mistral_answer': 'Subtracting, we get 6', 'model_judgment': 'incorrect answers', 'mathematical_correct': False}\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "from sympy import sympify, simplify\n",
    "import re\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "model_name = \"google/t5-small-ssm-nq\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# Example datasets\n",
    "dataset = [\n",
    "    {\"question\": \"What is 2 + 2?\", \"expected_answer\": \"4\", \"mistral_answer\": \"The answer is 4\"},\n",
    "    {\"question\": \"What is 3 * 3?\", \"expected_answer\": \"9\", \"mistral_answer\": \"When multiplying, 3 * 3 equals 9\"},\n",
    "    {\"question\": \"What is 10 - 3?\", \"expected_answer\": \"7\", \"mistral_answer\": \"Subtracting, we get 6\"}\n",
    "]\n",
    "\n",
    "# Function to extract and simplify mathematical expression\n",
    "def extract_math(expr):\n",
    "    \"\"\"\n",
    "    Extracts and simplifies mathematical expressions from strings,\n",
    "    removing all non-mathematical text and focusing on numbers.\n",
    "    \"\"\"\n",
    "    # Remove any non-numeric characters except for arithmetic symbols\n",
    "    expr = re.sub(r'[^\\d\\+\\-\\*/\\.^\\(\\)]+', '', expr)\n",
    "    try:\n",
    "        # Return the simplified expression\n",
    "        return simplify(sympify(expr))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Function to validate correctness using the model\n",
    "def validate_correctness(question, mistral_answer, expected_answer):\n",
    "    # Use T5 to classify the correctness of the Mistral model's output\n",
    "    prompt = (\n",
    "        f\"Evaluate if the given answer is correct:\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Mistral's Answer: {mistral_answer}\\n\"\n",
    "        f\"Expected Answer: {expected_answer}\\n\"\n",
    "        f\"Respond with 'Correct' or 'Incorrect'.\"\n",
    "    )\n",
    "\n",
    "    # Generate T5's judgment\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs.input_ids, max_length=10)\n",
    "    model_judgment = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Also validate mathematically\n",
    "    mistral_expr = extract_math(mistral_answer)\n",
    "    expected_expr = extract_math(expected_answer)\n",
    "    mathematical_correct = mistral_expr == expected_expr\n",
    "\n",
    "    return {\n",
    "        \"model_judgment\": model_judgment,\n",
    "        \"mathematical_correct\": mathematical_correct\n",
    "    }\n",
    "\n",
    "# Evaluate the dataset\n",
    "results = []\n",
    "for item in dataset:\n",
    "    result = validate_correctness(\n",
    "        question=item[\"question\"],\n",
    "        mistral_answer=item[\"mistral_answer\"],\n",
    "        expected_answer=item[\"expected_answer\"]\n",
    "    )\n",
    "    results.append({**item, **result})\n",
    "\n",
    "# Print results\n",
    "for res in results:\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt engineering doesn't work either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is 2 + 2?', 'expected_answer': '4', 'mistral_answer': 'The answer is 4', 'model_judgment': 'incorrect answers'}\n",
      "{'question': 'What is 3 * 3?', 'expected_answer': '9', 'mistral_answer': 'When multiplying, 3 * 3 equals 9', 'model_judgment': 'sets a real number'}\n",
      "{'question': 'What is 10 - 3?', 'expected_answer': '7', 'mistral_answer': 'Subtracting, we get 6', 'model_judgment': 'sets the answers'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "model_name = \"google/t5-small-ssm-nq\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# Example dataset\n",
    "dataset = [\n",
    "    {\"question\": \"What is 2 + 2?\", \"expected_answer\": \"4\", \"mistral_answer\": \"The answer is 4\"},\n",
    "    {\"question\": \"What is 3 * 3?\", \"expected_answer\": \"9\", \"mistral_answer\": \"When multiplying, 3 * 3 equals 9\"},\n",
    "    {\"question\": \"What is 10 - 3?\", \"expected_answer\": \"7\", \"mistral_answer\": \"Subtracting, we get 6\"}\n",
    "]\n",
    "\n",
    "# Function to validate correctness using the model\n",
    "def validate_correctness(question, mistral_answer, expected_answer):\n",
    "    # Construct the prompt to guide the model to compare only the mathematical part\n",
    "    prompt = (\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Expected Answer: {expected_answer}\\n\"\n",
    "        f\"Mistral's Answer: {mistral_answer}\\n\"\n",
    "        f\"Check if the provided answer is correct. Only provide 'Correct' or 'Incorrect' based on the numerical correctness of the final answer.\"\n",
    "    )\n",
    "\n",
    "    # Tokenize the prompt and generate the model's response\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs.input_ids, max_length=10)\n",
    "    model_judgment = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Return the model's judgment\n",
    "    return {\n",
    "        \"model_judgment\": model_judgment\n",
    "    }\n",
    "\n",
    "# Evaluate the dataset\n",
    "results = []\n",
    "for item in dataset:\n",
    "    result = validate_correctness(\n",
    "        question=item[\"question\"],\n",
    "        mistral_answer=item[\"mistral_answer\"],\n",
    "        expected_answer=item[\"expected_answer\"]\n",
    "    )\n",
    "    results.append({**item, **result})\n",
    "\n",
    "# Print results\n",
    "for res in results:\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EleutherAI/gpt-neo-2.7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changed model and worked on the prompt, but it wasn't able to correctly compare answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minettebrink/Documents/evaluation/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is 2 + 2?', 'expected_answer': '4', 'mistral_answer': 'The answer is 4', 'model_judgment': \"question: what is 2 + 2?\\nexpected answer: 4\\nanswer provided by mistral: the answer is 4\\n\\ncompare only the numeric value of the provided answer to the expected answer.\\ndoes the number in the provided answer exactly match the expected number?respond with 'yes' or 'no' only.\\n\\nquestion: what is 2 + 2?\", 'mathematical_correct': True}\n",
      "{'question': 'What is 3 * 3?', 'expected_answer': '9', 'mistral_answer': 'When multiplying, 3 * 3 equals 9', 'model_judgment': \"question: what is 3 * 3?\\nexpected answer: 9\\nanswer provided by mistral: when multiplying, 3 * 3 equals 9\\n\\ncompare only the numeric value of the provided answer to the expected answer.\\ndoes the number in the provided answer exactly match the expected number?respond with 'yes' or 'no' only.\\n\\nquestion: what is 3 * 3?\", 'mathematical_correct': True}\n",
      "{'question': 'What is 10 - 3?', 'expected_answer': '7', 'mistral_answer': 'Subtracting, we get 6', 'model_judgment': \"question: what is 10 - 3?\\nexpected answer: 7\\nanswer provided by mistral: subtracting, we get 6\\n\\ncompare only the numeric value of the provided answer to the expected answer.\\ndoes the number in the provided answer exactly match the expected number?respond with 'yes' or 'no' only.\\n\\nquestion: what is 10 - 3?\", 'mathematical_correct': True}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the evaluation model and tokenizer\n",
    "model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Example dataset\n",
    "dataset = [\n",
    "    {\"question\": \"What is 2 + 2?\", \"expected_answer\": \"4\", \"mistral_answer\": \"The answer is 4\"},\n",
    "    {\"question\": \"What is 3 * 3?\", \"expected_answer\": \"9\", \"mistral_answer\": \"When multiplying, 3 * 3 equals 9\"},\n",
    "    {\"question\": \"What is 10 - 3?\", \"expected_answer\": \"7\", \"mistral_answer\": \"Subtracting, we get 6\"}\n",
    "]\n",
    "\n",
    "# Function to evaluate correctness\n",
    "def evaluate_correctness(question, mistral_answer, expected_answer):\n",
    "    # Construct the prompt for numeric comparison\n",
    "    prompt = (\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Expected answer: {expected_answer}\\n\"\n",
    "        f\"Answer provided by Mistral: {mistral_answer}\\n\\n\"\n",
    "        \"Compare only the numeric value of the provided answer to the expected answer.\\n\"\n",
    "        \"Does the number in the provided answer exactly match the expected number?\"\n",
    "        \"Respond with 'Yes' or 'No' only.\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids, \n",
    "        max_new_tokens=10,  # Generate only up to 10 new tokens for concise output\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    \n",
    "    # Decode the model's response\n",
    "    model_response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip().lower()\n",
    "    \n",
    "    # Post-process the response to check for \"yes\" or \"no\"\n",
    "    mathematical_correct = \"yes\" in model_response\n",
    "    \n",
    "    return {\n",
    "        \"model_judgment\": model_response,\n",
    "        \"mathematical_correct\": mathematical_correct\n",
    "    }\n",
    "\n",
    "# Evaluate the dataset\n",
    "results = []\n",
    "for item in dataset:\n",
    "    result = evaluate_correctness(\n",
    "        question=item[\"question\"],\n",
    "        mistral_answer=item[\"mistral_answer\"],\n",
    "        expected_answer=item[\"expected_answer\"]\n",
    "    )\n",
    "    results.append({**item, **result})\n",
    "\n",
    "# Print the results\n",
    "for res in results:\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removed the extra text in the answer to see if it would help. It didn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is 2 + 2?', 'expected_answer': '4', 'mistral_answer': '4', 'model_judgment': \"question: what is 2 + 2?\\nexpected answer: 4\\nanswer provided by mistral: 4\\n\\ncompare only the numeric value of the provided answer to the expected answer.\\ndoes the number in the provided answer exactly match the expected number?respond with 'yes' or 'no' only.\\n\\nquestion: what is 2 + 2?\", 'mathematical_correct': True}\n",
      "{'question': 'What is 3 * 3?', 'expected_answer': '9', 'mistral_answer': '9', 'model_judgment': \"question: what is 3 * 3?\\nexpected answer: 9\\nanswer provided by mistral: 9\\n\\ncompare only the numeric value of the provided answer to the expected answer.\\ndoes the number in the provided answer exactly match the expected number?respond with 'yes' or 'no' only.\\n\\nquestion: what is 3 * 3?\", 'mathematical_correct': True}\n",
      "{'question': 'What is 10 - 3?', 'expected_answer': '7', 'mistral_answer': '6', 'model_judgment': \"question: what is 10 - 3?\\nexpected answer: 7\\nanswer provided by mistral: 6\\n\\ncompare only the numeric value of the provided answer to the expected answer.\\ndoes the number in the provided answer exactly match the expected number?respond with 'yes' or 'no' only.\\n\\nquestion: what is 10 - 3?\", 'mathematical_correct': True}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the evaluation model and tokenizer\n",
    "model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Example dataset\n",
    "dataset = [\n",
    "    {\"question\": \"What is 2 + 2?\", \"expected_answer\": \"4\", \"mistral_answer\": \"4\"},\n",
    "    {\"question\": \"What is 3 * 3?\", \"expected_answer\": \"9\", \"mistral_answer\": \"9\"},\n",
    "    {\"question\": \"What is 10 - 3?\", \"expected_answer\": \"7\", \"mistral_answer\": \"6\"}\n",
    "]\n",
    "\n",
    "# Function to evaluate correctness\n",
    "def evaluate_correctness(question, mistral_answer, expected_answer):\n",
    "    # Construct the prompt for numeric comparison\n",
    "    prompt = (\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Expected answer: {expected_answer}\\n\"\n",
    "        f\"Answer provided by Mistral: {mistral_answer}\\n\\n\"\n",
    "        \"Compare only the numeric value of the provided answer to the expected answer.\\n\"\n",
    "        \"Does the number in the provided answer exactly match the expected number?\"\n",
    "        \"Respond with 'Yes' or 'No' only.\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids, \n",
    "        max_new_tokens=10,  # Generate only up to 10 new tokens for concise output\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    \n",
    "    # Decode the model's response\n",
    "    model_response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip().lower()\n",
    "    \n",
    "    # Post-process the response to check for \"yes\" or \"no\"\n",
    "    mathematical_correct = \"yes\" in model_response\n",
    "    \n",
    "    return {\n",
    "        \"model_judgment\": model_response,\n",
    "        \"mathematical_correct\": mathematical_correct\n",
    "    }\n",
    "\n",
    "# Evaluate the dataset\n",
    "results = []\n",
    "for item in dataset:\n",
    "    result = evaluate_correctness(\n",
    "        question=item[\"question\"],\n",
    "        mistral_answer=item[\"mistral_answer\"],\n",
    "        expected_answer=item[\"expected_answer\"]\n",
    "    )\n",
    "    results.append({**item, **result})\n",
    "\n",
    "# Print the results\n",
    "for res in results:\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### google/flan-t5-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is 2 + 2?', 'expected_answer': '4', 'mistral_answer': 'The answer is 4', 'model_judgment': 'no', 'mathematical_correct': False}\n",
      "{'question': 'What is 3 * 3?', 'expected_answer': '9', 'mistral_answer': 'When multiplying, 3 * 3 equals 9', 'model_judgment': 'yes', 'mathematical_correct': True}\n",
      "{'question': 'What is 10 - 3?', 'expected_answer': '7', 'mistral_answer': 'Subtracting, we get 6', 'model_judgment': 'no', 'mathematical_correct': False}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load the FLAN-T5 model and tokenizer\n",
    "model_name = \"google/flan-t5-base\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Example dataset\n",
    "dataset = [\n",
    "    {\"question\": \"What is 2 + 2?\", \"expected_answer\": \"4\", \"mistral_answer\": \"The answer is 4\"},\n",
    "    {\"question\": \"What is 3 * 3?\", \"expected_answer\": \"9\", \"mistral_answer\": \"When multiplying, 3 * 3 equals 9\"},\n",
    "    {\"question\": \"What is 10 - 3?\", \"expected_answer\": \"7\", \"mistral_answer\": \"Subtracting, we get 6\"}\n",
    "]\n",
    "\n",
    "# Function to evaluate correctness\n",
    "def evaluate_correctness(question, mistral_answer, expected_answer):\n",
    "    # Construct the prompt\n",
    "    prompt = (\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Expected answer: {expected_answer}\\n\"\n",
    "        f\"Answer provided by Mistral: {mistral_answer}\\n\\n\"\n",
    "        \"Does the provided answer mean the same as the expected answer? \"\n",
    "        \"Respond with 'Yes' or 'No' only.\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=20,  # Generate only up to 20 new tokens\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    \n",
    "    # Decode the model's response\n",
    "    model_response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip().lower()\n",
    "    \n",
    "    # Determine correctness\n",
    "    mathematical_correct = \"yes\" in model_response\n",
    "    \n",
    "    return {\n",
    "        \"model_judgment\": model_response,\n",
    "        \"mathematical_correct\": mathematical_correct\n",
    "    }\n",
    "\n",
    "# Evaluate the dataset\n",
    "results = []\n",
    "for item in dataset:\n",
    "    result = evaluate_correctness(\n",
    "        question=item[\"question\"],\n",
    "        mistral_answer=item[\"mistral_answer\"],\n",
    "        expected_answer=item[\"expected_answer\"]\n",
    "    )\n",
    "    results.append({**item, **result})\n",
    "\n",
    "# Print the results\n",
    "for res in results:\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is 2 + 2?', 'expected_answer': '4', 'mistral_answer': 'The answer is 4', 'evaluation': False, 'model_judgment': 'no', 'evaluation_correct': False}\n",
      "{'question': 'What is 3 * 3?', 'expected_answer': '9', 'mistral_answer': 'When multiplying, 3 * 3 equals 9', 'evaluation': True, 'model_judgment': 'yes', 'evaluation_correct': True}\n",
      "{'question': 'What is 10 - 3?', 'expected_answer': '7', 'mistral_answer': 'Subtracting, we get 6', 'evaluation': False, 'model_judgment': 'no', 'evaluation_correct': False}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load the FLAN-T5 model and tokenizer\n",
    "model_name = \"google/flan-t5-large\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Example dataset\n",
    "dataset = [\n",
    "    {\"question\": \"What is 2 + 2?\", \"expected_answer\": \"4\", \"mistral_answer\": \"The answer is 4\", \"evaluation\": False},\n",
    "    {\"question\": \"What is 3 * 3?\", \"expected_answer\": \"9\", \"mistral_answer\": \"When multiplying, 3 * 3 equals 9\", \"evaluation\": True},\n",
    "    {\"question\": \"What is 10 - 3?\", \"expected_answer\": \"7\", \"mistral_answer\": \"Subtracting, we get 6\", \"evaluation\": False}\n",
    "]\n",
    "\n",
    "# Function to evaluate correctness of the evaluation\n",
    "def evaluate_correctness(question, mistral_answer, expected_answer, evaluation):\n",
    "    # Construct the prompt\n",
    "    prompt = (\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Expected answer: {expected_answer}\\n\"\n",
    "        f\"Answer provided by Mistral: {mistral_answer}\\n\"\n",
    "        f\"Evaluation says correctness is: {evaluation}\\n\\n\"\n",
    "        \"Is this evaluation correct based on the Mistral answer and expected answer? \"\n",
    "        \"Respond with 'Yes' or 'No' only.\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=20,  # Generate only up to 20 new tokens\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    \n",
    "    # Decode the model's response\n",
    "    model_response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip().lower()\n",
    "    \n",
    "    # Determine correctness\n",
    "    evaluation_correct = \"yes\" in model_response\n",
    "    \n",
    "    return {\n",
    "        \"model_judgment\": model_response,\n",
    "        \"evaluation_correct\": evaluation_correct\n",
    "    }\n",
    "\n",
    "# Evaluate the dataset\n",
    "results = []\n",
    "for item in dataset:\n",
    "    result = evaluate_correctness(\n",
    "        question=item[\"question\"],\n",
    "        mistral_answer=item[\"mistral_answer\"],\n",
    "        expected_answer=item[\"expected_answer\"],\n",
    "        evaluation=item[\"evaluation\"]\n",
    "    )\n",
    "    results.append({**item, **result})\n",
    "\n",
    "# Print the results\n",
    "for res in results:\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact match comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying different approches I came to a conclusion that in this case when I have a test data set. Which I don't know if it's good because it might be that, or most likely, that Mistrals Large model has been trained on it. But it is too time consuming to create a data set on my own. \n",
    "\n",
    "So I will extract the final numerical answer from the Mistral model and then compare them to the correct answers in the test data set. Based on that I can get some statistical measurments. \n",
    "\n",
    "Usage of smaller open source LLMs proved to be to unrealible compared to this approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from mistralai import Mistral\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key and agent ID\n",
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "agent_id = os.environ[\"MISTRAL_AGENT_ID\"]\n",
    "\n",
    "# Initialize Mistral client\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "# Load questions from the JSON file\n",
    "input_file = \"sample_questions.json\"  \n",
    "output_file = \"model_answers.json\"  # Output file name\n",
    "\n",
    "with open(input_file, \"r\") as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over the questions\n",
    "for question in questions:\n",
    "    try:\n",
    "        # Ask the question to the model\n",
    "        chat_response = client.agents.complete(\n",
    "            agent_id=agent_id,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": question\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "        # Extract the model's answer\n",
    "        answer = chat_response.choices[0].message.content.strip()\n",
    "\n",
    "        # Append the question and answer to the results\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "\n",
    "        #print(f\"Question: {question}\")\n",
    "        #print(f\"Answer: {answer}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question: {question}\")\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Save the results to the output file\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "#print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main problems is the format of the answer. When humans answer math questions we use logic and know that there are several ways to write the same thing. But when an AI answers the question it doesn't use logic, it is a probability machine. So, how can we get the right part from the out put extraceted, so that we don't need to evaluate the extraction as well. And, when we have answerd math problems it is never enough with the answer, we also need the resoning. How can we validate the reasoning of the answer. There are cases where the final answer is right but the reasoning is wrong? Like in a human case, you use the calculator to answer and guess how you come to the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to model_final_answers.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re  # Import re for regular expressions\n",
    "\n",
    "# Load the dataset\n",
    "with open(\"model_answers.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Function to extract the final numerical answer\n",
    "def extract_final_answer(entry):\n",
    "    answer_text = entry[\"answer\"]\n",
    "    # Use a regular expression to extract the number after \"The answer is\"\n",
    "    match = match = re.search(r\"(?:The answer is)\\s*(.*)\", answer_text)\n",
    "    if match:\n",
    "        # Extract and return the number as a float\n",
    "        final_answer = match.group(1)\n",
    "        return {\"question\": entry[\"question\"], \"answer\": final_answer}\n",
    "    else:\n",
    "        # If no match is found, return \"Answer not found\"\n",
    "        return {\"question\": entry[\"question\"], \"answer\": \"Answer not found\"}\n",
    "\n",
    "# Process each entry in the dataset\n",
    "processed_data = [extract_final_answer(entry) for entry in data]\n",
    "\n",
    "# Save the processed data to a new JSON file\n",
    "with open(\"model_final_answers.json\", \"w\") as file:\n",
    "    json.dump(processed_data, file, indent=4)\n",
    "\n",
    "print(\"Processed data saved to model_final_answers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison complete. Results saved to comparison_results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the two JSON files\n",
    "with open(\"model_final_answers.json\", \"r\") as model, open(\"sample_clean_qa_data.json\", \"r\") as correct:\n",
    "    data1 = json.load(model)\n",
    "    data2 = json.load(correct)\n",
    "\n",
    "# Create dictionaries for faster comparison\n",
    "answers1 = {entry[\"question\"]: entry[\"answer\"] for entry in data1}\n",
    "answers2 = {entry[\"question\"]: entry[\"answer\"] for entry in data2}\n",
    "\n",
    "# Compare answers for the same questions\n",
    "results = []\n",
    "for question in answers1:\n",
    "    if question in answers2:\n",
    "        match = answers1[question] == answers2[question]\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer_model\": answers1[question],\n",
    "            \"answer_correct\": answers2[question],\n",
    "            \"match\": match\n",
    "        })\n",
    "    else:\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer_model\": answers1[question],\n",
    "            \"answer_correct\": \"Not found in file2\",\n",
    "            \"match\": False\n",
    "        })\n",
    "\n",
    "# Check for questions in file2 but not in file1\n",
    "for question in answers2:\n",
    "    if question not in answers1:\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer_model\": \"Not found in file1\",\n",
    "            \"answer_correct\": answers2[question],\n",
    "            \"match\": False\n",
    "        })\n",
    "\n",
    "# Save the results to a new JSON file\n",
    "with open(\"comparison_results.json\", \"w\") as result_file:\n",
    "    json.dump(results, result_file, indent=4)\n",
    "\n",
    "print(\"Comparison complete. Results saved to comparison_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
